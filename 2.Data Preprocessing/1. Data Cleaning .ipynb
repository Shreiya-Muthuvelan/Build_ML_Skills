{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN8v-VLB8s4S"
      },
      "source": [
        "## **Data Cleaning**\n",
        "\n",
        "In this notebook, we’ll go through the **first step of data preprocessing — cleaning the data**\n",
        "\n",
        "Real-world datasets are rarely perfect. They often contain:\n",
        "\n",
        "- Missing values (e.g., a person’s age not recorded)\n",
        "\n",
        "- Duplicates (same record appearing more than once)\n",
        "\n",
        "- Inconsistent values (typos, wrong formats)\n",
        "\n",
        "- Outliers (unusual data points that may affect the model)\n",
        "\n",
        "If we don’t fix these issues, machine learning models may learn incorrect patterns and give poor results.\n",
        "\n",
        "What we’ll do in this notebook\n",
        "\n",
        "- Detect and handle missing values\n",
        "\n",
        "- Remove duplicates\n",
        "\n",
        "- Identify and treat outliers\n",
        "\n",
        "- Make the dataset consistent and ready for analysis\n",
        "\n",
        "By the end, we’ll have a clean, reliable dataset that can be used for further preprocessing and modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88QYGYpT86Ue",
        "outputId": "3198194d-f880-4ba2-fe6b-9d2efde9337a"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "\n",
        "adult = fetch_openml(name=\"adult\", version=2, as_frame=True)\n",
        "\n",
        "df_adult = adult.frame\n",
        "\n",
        "df_adult.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHu9072eehQz",
        "outputId": "6d5d1736-b60a-489f-8de0-995b46c129b0"
      },
      "outputs": [],
      "source": [
        "df_adult.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "la51mdESd7-A",
        "outputId": "dfb81d25-1034-4526-abb2-b04a358013ed"
      },
      "outputs": [],
      "source": [
        "# Check first rows\n",
        "df_adult.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_mE2i49r_wH"
      },
      "source": [
        "It is given in the documentation for the dataset that fnlwgt is a sampling column(not an actual feature). Therefore its better to drop it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4ecVzvpro94"
      },
      "outputs": [],
      "source": [
        "df_adult=df_adult.drop(columns=['fnlwgt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Zv5E2heH7C"
      },
      "source": [
        "### **Handling Missing Values**\n",
        "\n",
        "In real-world datasets, it’s very common to have some missing or blank entries. For example, in a survey dataset, a person might choose not to answer a question like “income” or “workclass.”\n",
        "\n",
        "Why does it matter?\n",
        "\n",
        "- Most machine learning algorithms cannot handle missing values directly.\n",
        "\n",
        "- If we ignore them, the model may fail to train or give unreliable results.\n",
        "\n",
        "- Too many missing values can reduce the amount of usable data, which weakens the model.\n",
        "\n",
        "So, instead of just leaving them as blanks, we use different strategies to handle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua4rvhqak5Gj"
      },
      "source": [
        "**How do we handle them?**\n",
        "\n",
        "There is no “one-size-fits-all” method—it depends on the amount and type of missing data. Some common strategies are:\n",
        "\n",
        "- Drop rows or columns\n",
        "\n",
        "    - If only a very small number of rows are missing values, we can remove them without losing much information.\n",
        "\n",
        "    - Similarly, if an entire column has too many missing values, sometimes it’s better to drop it.\n",
        "\n",
        "- Imputation (filling in values)\n",
        "\n",
        "    - Numerical features: Fill missing values with the mean or median of the column.\n",
        "\n",
        "        - Mean is good when the data is evenly distributed.\n",
        "\n",
        "        - Median is better if the column has outliers.\n",
        "\n",
        "    - Categorical features: Replace missing values with the most frequent (mode) value.\n",
        "\n",
        "- Special category for missing values\n",
        "\n",
        "    - For categorical columns, we can add a new label like \"Unknown\" or \"Missing\".\n",
        "\n",
        "    - This is useful when missingness itself might carry information (e.g., people not reporting occupation could have a pattern)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "8F7a_YUgeUxF",
        "outputId": "4648bcd7-2194-44d4-9200-b59d650d494f"
      },
      "outputs": [],
      "source": [
        "df_adult.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3KZ1HHEemO8"
      },
      "source": [
        "Applying it to this dataset\n",
        "\n",
        "- native-country → Has about 857 missing values (~2% of data). This is small, so we can safely fill it with the most common country (mode).\n",
        "\n",
        "- workclass and occupation → Each has ~6% missing values. Instead of dropping these rows (which would throw away a lot of data), we can fill them with a new category \"Unknown\". This way, we don’t lose information, and the model can learn from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUGmKiPWl2K9"
      },
      "outputs": [],
      "source": [
        "df_adult['workclass'] = df_adult['workclass'].astype('object')\n",
        "df_adult['occupation'] = df_adult['occupation'].astype('object')\n",
        "\n",
        "df_adult['workclass'] = df_adult['workclass'].fillna('Unknown')\n",
        "df_adult['occupation'] = df_adult['occupation'].fillna('Unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an-KAU8bmYei"
      },
      "outputs": [],
      "source": [
        "df_adult['native-country'] = df_adult['native-country'].fillna(df_adult['native-country'].mode()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdBsb8qWmfEC"
      },
      "source": [
        "### **Removing Duplicate Values**\n",
        "\n",
        "Sometimes, datasets contain duplicate rows, meaning the exact same entry appears more than once.\n",
        "For example, if two identical records of the same person are present, the dataset is giving them more importance than others.\n",
        "\n",
        "**Why does it matter?**\n",
        "\n",
        "- Duplicates can bias the model by giving extra weight to certain records.\n",
        "\n",
        "- This reduces fairness and may affect predictions.\n",
        "\n",
        "- Cleaning duplicates ensures every observation is unique and treated equally.\n",
        "\n",
        "**How do we handle them?**\n",
        "\n",
        "- Detect and remove duplicates.\n",
        "\n",
        "- Keep only the first occurrence (or the most relevant one if the dataset has timestamps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB80c38Cmyyf"
      },
      "outputs": [],
      "source": [
        "df_adult.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SITm3eZUm2CK",
        "outputId": "b4787b3c-32ec-4a91-ae23-fbfe8a8d9d89"
      },
      "outputs": [],
      "source": [
        "df_adult.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgrjdYbcm57c"
      },
      "source": [
        "### **Detecting Outliers**\n",
        "\n",
        "**What are outliers?**\n",
        "Outliers are extreme values that don’t fit the normal pattern of data. For example:\n",
        "\n",
        "- A person working 200 hours per week\n",
        "\n",
        "- Someone with an income of 10 million in a dataset where most incomes are under 100k\n",
        "\n",
        "Outliers can appear for two reasons:\n",
        "\n",
        "- **Errors in data entry or measurement** → e.g., a person’s age recorded as 500.\n",
        "\n",
        "- **Genuine rare cases** → e.g., someone earning very high income compared to others.\n",
        "\n",
        "If outliers are errors, they should be removed. But if they are real values, they might still carry important information. That’s why we need to carefully analyze outliers before deciding what to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvEP-UKio_sQ"
      },
      "source": [
        "#### **Method 1 : Data Visualization**\n",
        "The easiest way to spot outliers is by visualizing the data (for example, using a boxplot or histogram).\n",
        "\n",
        "- A boxplot shows the spread of data and highlights values that fall far from the rest.\n",
        "\n",
        "- For example, when plotting the age column, we can quickly see if some ages are much higher or lower than most others.\n",
        "\n",
        "Visualization helps us get an intuition about whether unusual values look like mistakes or valid rare cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "djG9md_bo-7w",
        "outputId": "df11a13b-1079-42ae-e303-d2462e678706"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.boxplot(x=df_adult['age'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see here that people in the age group 80–90 are flagged as outliers. This happens because they are relatively rare compared to the rest of the population.\n",
        "\n",
        "However, this is not a data entry mistake—it’s simply a valid but less common case (some people really do live and work into their 80s and 90s).\n",
        "\n",
        "In such situations, we keep the values instead of removing them, since they represent real-world diversity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGRNxKsnpgL4"
      },
      "source": [
        "#### **Method 2: Using Summary Statistics**\n",
        "Another common way to detect outliers is by using the **Interquartile Range (IQR)**.\n",
        "- Calculate the Interquartile Range \n",
        "\n",
        "$$\n",
        "\\text{IQR} = Q3 – Q1\n",
        "$$ \n",
        "\n",
        "- Define thresholds:\n",
        "\n",
        "$$\n",
        "\\text{Lower bound}= Q1-  1.5 X  IQR\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Upper bound}=Q3+ 1.5 XIQR\n",
        "$$\n",
        "\n",
        "Any value outside this range is considered an outlier\n",
        "\n",
        "This method is useful because it works for many distributions without making assumptions about the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF2MQjkbn55P",
        "outputId": "2cef8e7f-bee8-49a8-b748-b4d3eaaa4938"
      },
      "outputs": [],
      "source": [
        "numeric_cols = df_adult.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = df_adult[col].quantile(0.25)\n",
        "    Q3 = df_adult[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = df_adult[(df_adult[col] < lower) | (df_adult[col] > upper)]\n",
        "    print(f\"{col}: {outliers.shape[0]} outliers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2V9CGAbrMQf"
      },
      "source": [
        "**Column-wise Observations**\n",
        "\n",
        "- **hours-per-week:**\n",
        "Most people work around 40 hours per week, but some work 60–90. These are not mistakes — they are valid, just less common. We should keep them.\n",
        "\n",
        "- **education-num:**\n",
        "This column represents years of education, mapped into categories between 1 and 16. Since the range is fixed, values outside this are unlikely. Outliers here are usually not errors but edge cases.\n",
        "\n",
        "- **capital-gain:**\n",
        "Highly skewed — most people have 0, but a few have very high values. These are genuine rare high-income cases. Dropping them would lose important information.\n",
        "\n",
        "- **capital-loss:**\n",
        "Same reasoning as capital-gain. Most values are 0, with a few large ones that are valid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFepdtrEumb7"
      },
      "source": [
        "**What to do with outliers?**\n",
        "\n",
        "- **In many datasets:** Outliers are dropped because they can add noise and reduce model accuracy.\n",
        "\n",
        "- **In this dataset:** Some “outliers” are actually real rare cases (e.g., high capital gains/losses, long working hours). Dropping them would remove valuable information.\n",
        "\n",
        "Instead of dropping, we apply a log transformation:\n",
        "\n",
        "**Log Transformation**\n",
        "\n",
        "- Log reduces the scale of very large numbers while keeping their relative differences intact.\n",
        "\n",
        "- Example: instead of values like 0, 5000, 10000, 20000, after log we get much smaller, compressed values.\n",
        "\n",
        "- This way, the influence of extreme values is reduced, but we don’t lose the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Final Note:** Outlier handling is not about blindly removing unusual values. It’s about understanding the data and deciding whether those values are mistakes or important rare cases. In our case, we keep them and use log transformation to make the model handle them better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2M_4KF5rLd_",
        "outputId": "bd0b5da5-7fd7-4de6-e873-2de0be485859"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(df_adult['capital-gain'].describe())\n",
        "\n",
        "# Apply log transform\n",
        "df_adult['capital-gain'] = np.log1p(df_adult['capital-gain'])\n",
        "\n",
        "print(df_adult['capital-gain'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg9BFJ2XvbOj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(df_adult['capital-loss'].describe())\n",
        "\n",
        "# Apply log transform\n",
        "df_adult['capital-loss'] = np.log1p(df_adult['capital-loss'])\n",
        "\n",
        "print(df_adult['capital-loss'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6XLN0EdxzrEk",
        "outputId": "b1263ccb-699b-4b6c-aa25-efda571718e7"
      },
      "outputs": [],
      "source": [
        "df_adult.to_csv(\"adult_cleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now our dataset is cleaned - with missing values handled, duplicates removed and outliers treated- the data is trustworthy. The next step is to perform some **exploratory data analysis** to understand our data and gain insights from it "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
